{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from load_data import load_images\n",
    "from losses import content_loss, style_loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from transforms import prep, post\n",
    "from torch.autograd import Variable\n",
    "from macros import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Current Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = load_images(1, 2)\n",
    "imgs = [prep(img) for img in imgs]\n",
    "imgs = [Variable(img.unsqueeze(0).to(device)) for img in imgs]\n",
    "img_con, img_sty = imgs\n",
    "opt_img = Variable(img_con.data.clone(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing from content image... Using pre-trained model VGG19_bn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayo/miniconda3/envs/kAI/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/dayo/miniconda3/envs/kAI/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "print('Optimizing from content image... Using pre-trained model VGG19_bn')\n",
    "model = models.vgg19_bn(pretrained=True).to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSaver(nn.Module):\n",
    "    feature = None\n",
    "    def __init__(self, layer):\n",
    "        self.hook = layer.register_forward_hook(self.hook_func)\n",
    "    def hook_func(self, module, input, output):\n",
    "        self.feature = output\n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content features from layer 37 of the model\n",
      "<generator object <genexpr> at 0x16282d0e0>\n"
     ]
    }
   ],
   "source": [
    "content_feature_savers = [FeatureSaver(model.features[layer]) for layer in content_layers]\n",
    "model(Variable(img_con))\n",
    "content_features = [saver.feature.clone() for saver in content_feature_savers]\n",
    "print(f'Saved content features from layer {content_layers[0]} of the model')\n",
    "style_feature_savers  = [FeatureSaver(model.features[layer]) for layer in style_layers]\n",
    "model(Variable(img_sty))\n",
    "style_features = [saver.feature.clone() for saver in style_feature_savers]\n",
    "print(f'Saved style features from layer {layer} of the model' for layer in style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.LBFGS([opt_img])\n",
    "\n",
    "def closure():\n",
    "    global i\n",
    "    model(opt_img)\n",
    "    gen_content_feats = [saver.feature.clone() for saver in content_feature_savers]\n",
    "    gen_style_feats = [saver.feature.clone() for saver in style_feature_savers]\n",
    "\n",
    "    contentloss = WEIGHT_CONTENT * content_loss(gen_content_feats, content_features)\n",
    "    styleloss = style_loss(gen_style_feats, style_features, WEIGHTS_STYLE)\n",
    "    loss = contentloss + styleloss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    if i % show_iter == 0:\n",
    "        print(f\"Epoch: {i}, Content loss: {contentloss}, Style loss: {styleloss}, Total loss: {loss}\")\n",
    "    i += 1\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "Epoch: 0, Content loss: 0.0, Style loss: 188437.03125, Total loss: 188437.03125\n",
      "Epoch: 50, Content loss: 0.0058092097751796246, Style loss: 2942.35205078125, Total loss: 2942.35791015625\n",
      "Epoch: 100, Content loss: 0.006294267252087593, Style loss: 638.6104125976562, Total loss: 638.61669921875\n",
      "Epoch: 150, Content loss: 0.006473978981375694, Style loss: 263.77642822265625, Total loss: 263.78289794921875\n",
      "Epoch: 200, Content loss: 0.006575554143637419, Style loss: 141.4736328125, Total loss: 141.48020935058594\n",
      "Epoch: 250, Content loss: 0.006645968183875084, Style loss: 93.17888641357422, Total loss: 93.18553161621094\n",
      "Epoch: 300, Content loss: 0.006698772311210632, Style loss: 66.3231201171875, Total loss: 66.32981872558594\n",
      "Epoch: 350, Content loss: 0.006747280713170767, Style loss: 51.66287612915039, Total loss: 51.66962432861328\n",
      "Epoch: 400, Content loss: 0.0067830318585038185, Style loss: 42.37866973876953, Total loss: 42.38545227050781\n",
      "Epoch: 450, Content loss: 0.006819332484155893, Style loss: 35.7735710144043, Total loss: 35.780391693115234\n",
      "Training completed in 1167.82 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "i = 0\n",
    "print('Start Training...')\n",
    "while i < max_iter:\n",
    "    optimizer.step(closure)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "out_img = post(opt_img.data[0].cpu().squeeze())\n",
    "out_img.save('Result/kanagawa_neckarfront.png', format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
